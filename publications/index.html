<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google analytics tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-CYGH5M4NVH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-CYGH5M4NVH');
  </script>

  <!-- Metadata -->
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="robots" content="noarchive">

  <title>Publications &mdash; Ellis Michael</title>
  
  <meta name="author" content="Ellis Michael">

  <link rel="canonical" href="https://ellismichael.com/publications/">
  <link rel="shortcut icon" type="image/ico" href="/favicon.ico">
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml">

  <!-- Custom fonts -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/academicons/css/academicons.min.css"/>

  <!-- Custom CSS -->
  <link href="/css/main.css" rel="stylesheet">
</head>

<body>
  <header>
    <div class="row">
      <div class="title-wrapper">
        <h1>Ellis Michael</h1>
      </div>
    </div>
    <div class="row">
      <div class="nav-wrapper"><nav><ul>
        
          <li>
            <a href="/">Home</a>
          </li>
        
          <li class="active">
            <a href="/publications/">Publications</a>
          </li>
        
          <li>
            <a href="/code/">Code</a>
          </li>
        
          <li>
            <a href="/cv/">CV</a>
          </li>
        
      </ul></nav></div>
    </div>
  </header>

  <main>
    <div class="row">
  <div class="std-content-wrapper">
    <div>
<a class="bibtex-download" href="/publications.bib"><i class="fa fa-download"></i> BibTeX</a>
<h3>Publications</h3>
</div>

<table class="page-pubs">
<tbody>



  <tr>
    <td>
      <span class="date">
        
          <span class="year">2023</span><br>
          
          <span class="month">April</span>
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Hydra: Serialization-Free Network Ordering for Strongly Consistent Distributed Applications.</span><br>
      <span class="authors">Inho Choi, Ellis Michael, Yunfan Li, Dan R. K. Ports, and Jialin Li.
      
      </span>
      <br>

      
        <span class="venue">Proceedings of the 20th USENIX Symposium on Networked Systems Design and Implementation (</span><span class="venueshort">NSDI '23).</span>
         <span class=venuelocation>Boston, MA, USA.</span> 
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_hydra-nsdi23">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://ellismichael.com/papers/hydra-nsdi23.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_hydra-nsdi23" class="abstract">
          Many distributed systems, e.g., state machine replication and distributed databases, rely on establishing a consistent order of operations on groups of nodes in the system. Traditionally, this ordering has been established by application-level protocols like Paxos or two-phase locking. Recent work has shown significant performance improvements are attainable by making ordering a network service, but current network sequencing implementations require routing all requests through a single sequencer – leading to scalability, fault tolerance, and load balancing limitations.<br />
Our work, Hydra, overcomes these limitations by using a distributed set of network sequencers to provide network ordering. Hydra leverages loosely synchronized clocks on network sequencers to establish message ordering across them, per-sequencer sequence numbers to detect message drops, and periodic timestamp messages to enforce progress when some sequencers are idle. To demonstrate the benefit of Hydra, we co-designed a state machine replication protocol and a distributed transactional system using the Hydra network primitive. Compared to serialization-based network ordering systems, Hydra shows equivalent performance improvement over traditional approaches in both applications, but with significantly higher scalability, shorter sequencer failover time, and better network-level load balancing.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          <span class="year">2020</span><br>
          
          <span class="month">November</span>
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Pegasus: Tolerating Skewed Workloads in Distributed Storage with In-Network Coherence Directories.</span><br>
      <span class="authors">Jialin Li, Jacob Nelson, Ellis Michael, Xin Jin, and Dan R. K. Ports.
      
      </span>
      <br>

      
        <span class="venue">Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation (</span><span class="venueshort">OSDI '20).</span>
         <span class=venuelocation>Banff, Alberta, Canada.</span> 
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_pegasus-osdi20">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://ellismichael.com/papers/pegasus-osdi20.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_pegasus-osdi20" class="abstract">
          High performance distributed storage systems face the challenge of load imbalance caused by skewed and dynamic workloads. This paper introduces Pegasus, a new storage system that leverages new-generation programmable switch ASICs to balance load across storage servers. Pegasus uses selective replication of the most popular objects in the data store to distribute load. Using a novel in-network coherence directory, the Pegasus switch tracks and manages the location of replicated objects. This allows it to achieve load-aware forwarding and dynamic rebalancing for replicated keys, while still guaranteeing data coherence and consistency. The Pegasus design is practical to implement as it stores only forwarding metadata in the switch data plane. The resulting system improves the 99% tail latency of a distributed in-memory key-value store by more than 95%, and yields up to a 9x throughput improvement under a latency SLO – results which hold across a large set of workloads with varying degrees of skew, read/write ratio, and dynamism.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          
            <span class="month">August</span>
            
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">On the Use of Model Checking for Remote Instruction.</span><br>
      <span class="authors">Tom Anderson and Ellis Michael.
      
      </span>
      <br>

      
        <span class="venue">ACM SIGCOMM CCR Series on Networking Education.</span>
        
      

      
      <br>
      <span class="links">
        

        
          <a href="https://ellismichael.com/papers/mchecking-sigcommedu20.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Harmonia: Near-Linear Scalability for Replicated Storage with In-Network Conflict Detection.</span><br>
      <span class="authors">Hang Zhu, Zhihao Bai, Jialin Li, Ellis Michael, Dan R. K. Ports, Ion Stoica, and Xin Jin.
      
      </span>
      <br>

      
        <span class="venue">Proceedings of the 46th International Conference on Very Large Data Bases (</span><span class="venueshort">VLDB '20).</span>
         <span class=venuelocation>Tokyo, Japan.</span> 
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_harmonia-vldb20">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://ellismichael.com/papers/harmonia-vldb19.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_harmonia-vldb20" class="abstract">
          Distributed storage employs replication to mask failures and improve availability. However, these systems typically exhibit a hard tradeoff between consistency and performance. Ensuring consistency introduces coordination overhead, and as a result the system throughput does not scale with the number of replicas. We present Harmonia, a replicated storage architecture that exploits the capability of new-generation programmable switches to obviate this tradeoff by providing near-linear scalability without sacrificing consistency. To achieve this goal, Harmonia detects read-write conflicts in the network, which enables any replica to serve reads for objects with no pending writes. Harmonia implements this functionality at line rate, thus imposing no performance overhead. We have implemented a prototype of Harmonia on a cluster of commodity servers connected by a Barefoot Tofino switch, and have integrated it with Redis. We demonstrate the generality of our approach by supporting a variety of replication protocols, including primary-backup, chain replication, Viewstamped Replication, and NOPaxos. Experimental results show that Harmonia improves the throughput of these protocols by up to 10x for a replication factor of 10, providing near-linear scalability up to the limit of our testbed.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          <span class="year">2019</span><br>
          
          <span class="month">April</span>
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Harmonia: Near-Linear Scalability for Replicated Storage with In-Network Conflict Detection.</span><br>
      <span class="authors">Hang Zhu, Zhihao Bai, Jialin Li, Ellis Michael, Dan R. K. Ports, Ion Stoica, and Xin Jin.
      
      </span>
      <br>

      
        <span class="venuetype">Technical Report</span>
        <span class="venue">1904.08964,</span>
        <span class=venuelocation>arXiv.</span>
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_harmonia-arxiv19">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://arxiv.org/pdf/1904.08964.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_harmonia-arxiv19" class="abstract">
          Distributed storage employs replication to mask failures and improve availability. However, these systems typically exhibit a hard tradeoff between consistency and performance. Ensuring consistency introduces coordination overhead, and as a result the system throughput does not scale with the number of replicas. We present Harmonia, a replicated storage architecture that exploits the capability of new-generation programmable switches to obviate this tradeoff by providing near-linear scalability without sacrificing consistency. To achieve this goal, Harmonia detects read-write conflicts in the network, which enables any replica to serve reads for objects with no pending writes. Harmonia implements this functionality at line rate, thus imposing no performance overhead. We have implemented a prototype of Harmonia on a cluster of commodity servers connected by a Barefoot Tofino switch, and have integrated it with Redis. We demonstrate the generality of our approach by supporting a variety of replication protocols, including primary-backup, chain replication, Viewstamped Replication, and NOPaxos. Experimental results show that Harmonia improves the throughput of these protocols by up to 10x for a replication factor of 10, providing near-linear scalability up to the limit of our testbed.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          
            <span class="month">March</span>
            
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Teaching Rigorous Distributed Systems With Efficient Model Checking.</span><br>
      <span class="authors">Ellis Michael, Doug Woos, Thomas Anderson, Michael D. Ernst, and Zachary Tatlock.
      
      </span>
      <br>

      
        <span class="venue">Proceedings of the 14th European Conference on Computer Systems (</span><span class="venueshort">EuroSys '19).</span>
         <span class=venuelocation>Dresden, Germany.</span> 
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_dslabs-eurosys19">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://ellismichael.com/papers/dslabs-eurosys19.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        
          <a href="https://ellismichael.com/papers/dslabs-eurosys19-slides.pdf">
              <i class="fa fa-files-o"></i> slides
          </a>
        

        
      </span>

      
        <div id="abstract_dslabs-eurosys19" class="abstract">
          Writing correct distributed systems code is difficult, especially for novice programmers. The inherent asynchrony and need for fault-tolerance make errors almost inevitable. Industrial-strength testing and model checking have been shown to be effective at uncovering bugs, but they come at a cost ― in both time and effort ― that is far beyond what students can afford. To address this, we have developed an efficient model checking framework and visual debugger for distributed systems, with the goal of helping students find and fix bugs in near real-time. We identify two novel techniques for reducing the search state space to more efficiently find bugs in student implementations. We report our experiences using these tools to help over two hundred students build a correct, linearizable, fault-tolerant, dynamically-sharded key–value store.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          <span class="year">2018</span><br>
          
          <span class="month">April</span>
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Towards Causal Datacenter Networks.</span><br>
      <span class="authors">Ellis Michael and Dan R. K. Ports.
      
      </span>
      <br>

      
        <span class="venue">Proceedings of the 5th Workshop on Principles and Practice of Consistency for Distributed Data (</span><span class="venueshort">PaPoC '18).</span>
         <span class=venuelocation>Porto, Portugal.</span> 
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_tcdn-papoc18">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://syslab.cs.washington.edu/papers/tcdn-papoc18.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_tcdn-papoc18" class="abstract">
          Traditionally, distributed systems conservatively assume an asynchronous network. However, recent work on the co-design of networks and distributed systems has shown that stronger ordering properties are achievable in datacenter networks and yield performance improvements for the distributed systems they support. We build on that trend and ask whether it is possible for the datacenter network to order all messages in a protocol-agnostic way. This approach, which we call omnisequencing, would ensure causal delivery of all messages, making consistency a network-level guarantee.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          <span class="year">2017</span><br>
          
          <span class="month">October</span>
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Eris: Coordination-Free Consistent Transactions Using In-Network Concurrency Control.</span><br>
      <span class="authors">Jialin Li, Ellis Michael, and Dan R. K. Ports.
      
      </span>
      <br>

      
        <span class="venue">Proceedings of the 26th ACM Symposium on Operating Systems Principles (</span><span class="venueshort">SOSP '17).</span>
         <span class=venuelocation>Shanghai, China.</span> 
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_eris-sosp17">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://syslab.cs.washington.edu/papers/eris-sosp17.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        
          <a href="https://syslab.cs.washington.edu/papers/eris-sosp17-slides.pdf">
              <i class="fa fa-files-o"></i> slides
          </a>
        

        
          <a href="https://dl.acm.org/ft_gateway.cfm?id=3132751&ftid=1940567&dwn=1">
              <i class="fa fa-film"></i> video
          </a>
        
      </span>

      
        <div id="abstract_eris-sosp17" class="abstract">
          Distributed storage systems aim to provide strong consistency and isolation guarantees on an architecture that is partitioned across multiple shards for scalability and replicated for fault tolerance. Traditionally, achieving all of these goals has required an expensive combination of atomic commitment and replication protocols – introducing extensive coordination overhead. Our system, Eris, takes a different approach. It moves a core piece of concurrency control functionality, which we term multi-sequencing, into the datacenter network itself. This network primitive takes on the responsibility for consistently ordering transactions, and a new lightweight transaction protocol ensures atomicity.<br />
The end result is that Eris avoids both replication and transaction coordination overhead: we show that it can process a large class of distributed transactions in a single round-trip from the client to the storage system without any explicit coordination between shards or replicas in the normal case. It provides atomicity, consistency, and fault tolerance with less than 10% overhead – achieving throughput 3.6–35x higher and latency 72–80% lower than a conventional design on standard benchmarks.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Eris: Coordination-Free Consistent Transactions Using In-Network Concurrency Control (Extended Version).</span><br>
      <span class="authors">Jialin Li, Ellis Michael, and Dan R. K. Ports.
      
      </span>
      <br>

      
        <span class="venuetype">Technical Report</span>
        <span class="venue">UW-CSE-17-10-01,</span>
        <span class=venuelocation>University of Washington CSE.</span>
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_eris-tr17">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://www.cs.washington.edu/tr/2017/10/UW-CSE-17-10-01.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_eris-tr17" class="abstract">
          Distributed storage systems aim to provide strong consistency and isolation guarantees on an architecture that is partitioned across multiple shards for scalability and replicated for fault tolerance. Traditionally, achieving all of these goals has required an expensive combination of atomic commitment and replication protocols – introducing extensive coordination overhead. Our system, Eris, takes a different approach. It moves a core piece of concurrency control functionality, which we term multi-sequencing, into the datacenter network itself. This network primitive takes on the responsibility for consistently ordering transactions, and a new lightweight transaction protocol ensures atomicity.<br />
The end result is that Eris avoids both replication and transaction coordination overhead: we show that it can process a large class of distributed transactions in a single round-trip from the client to the storage system without any explicit coordination between shards or replicas in the normal case. It provides atomicity, consistency, and fault tolerance with less than 10% overhead – achieving throughput 3.6–35x higher and latency 72–80% lower than a conventional design on standard benchmarks.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Recovering Shared Objects Without Stable Storage.</span><br>
      <span class="authors">Ellis Michael, Dan R. K. Ports, Naveen Kr. Sharma, and Adriana Szekeres.
      
      </span>
      <br>

      
        <span class="venue">Proceedings of the 31st International Symposium on Distributed Computing (</span><span class="venueshort">DISC '17).</span>
         <span class=venuelocation>Vienna, Austria.</span> 
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_recovering-disc17">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://syslab.cs.washington.edu/papers/recovering-disc17.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_recovering-disc17" class="abstract">
          This paper considers the problem of building fault-tolerant shared objects when processes can crash and recover but lose their persistent state on recovery. This Diskless Crash-Recovery (DCR) model matches the way many long-lived systems are built. We show that it presents new challenges, as operations that are recorded at a quorum may not persist after some of the processes in that quorum crash and then recover.<br />
To address this problem, we introduce the notion of crash-consistent quorums, where no recoveries happen during the quorum responses. We show that relying on crash-consistent quorums enables a recovery procedure that can recover all operations that successfully finished. Crash-consistent quorums can be easily identified using a mechanism we term the crash vector, which tracks the causal relationship between crashes, recoveries, and other operations.<br />
We apply crash-consistent quorums and crash vectors to build two storage primitives. We give a new algorithm for multi-writer, multi-reader atomic registers in the DCR model that guarantees safety under all conditions and termination under a natural condition. It improves on the best prior protocol for this problem by requiring fewer rounds, fewer nodes to participate in the quorum, and a less restrictive liveness condition. We also present a more efficient single-writer, single-reader atomic set―a virtual stable storage abstraction. It can be used to lift any existing algorithm from the traditional Crash-Recovery model to the DCR model. We examine a specific application, state machine replication, and show that existing diskless protocols can violate their correctness guarantees, while ours offers a general and correct solution.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          
            <span class="month">August</span>
            
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Recovering Shared Objects Without Stable Storage (Extended Version).</span><br>
      <span class="authors">Ellis Michael, Dan R. K. Ports, Naveen Kr. Sharma, and Adriana Szekeres.
      
      </span>
      <br>

      
        <span class="venuetype">Technical Report</span>
        <span class="venue">UW-CSE-17-08-01,</span>
        <span class=venuelocation>University of Washington CSE.</span>
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_recovering-tr17">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://www.cs.washington.edu/tr/2017/08/UW-CSE-17-08-01.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_recovering-tr17" class="abstract">
          This paper considers the problem of building fault-tolerant shared objects when processes can crash and recover but lose their persistent state on recovery. This Diskless Crash-Recovery (DCR) model matches the way many long-lived systems are built. We show that it presents new challenges, as operations that are recorded at a quorum may not persist after some of the processes in that quorum crash and then recover.<br />
To address this problem, we introduce the notion of crash-consistent quorums, where no recoveries happen during the quorum responses. We show that relying on crash-consistent quorums enables a recovery procedure that can recover all operations that successfully finished. Crash-consistent quorums can be easily identified using a mechanism we term the crash vector, which tracks the causal relationship between crashes, recoveries, and other operations.<br />
We apply crash-consistent quorums and crash vectors to build two storage primitives. We give a new algorithm for multi-writer, multi-reader atomic registers in the DCR model that guarantees safety under all conditions and termination under a natural condition. It improves on the best prior protocol for this problem by requiring fewer rounds, fewer nodes to participate in the quorum, and a less restrictive liveness condition. We also present a more efficient single-writer, single-reader atomic set―a virtual stable storage abstraction. It can be used to lift any existing algorithm from the traditional Crash-Recovery model to the DCR model. We examine a specific application, state machine replication, and show that existing diskless protocols can violate their correctness guarantees, while ours offers a general and correct solution.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          <span class="year">2016</span><br>
          
          <span class="month">November</span>
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Just Say NO to Paxos Overhead: Replacing Consensus with Network Ordering.</span><br>
      <span class="authors">Jialin Li, Ellis Michael, Adriana Szekeres, Naveen Kr. Sharma, and Dan R. K. Ports.
      
      </span>
      <br>

      
        <span class="venue">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (</span><span class="venueshort">OSDI '16).</span>
         <span class=venuelocation>Savannah, GA, USA.</span> 
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_nopaxos-osdi16">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://syslab.cs.washington.edu/papers/nopaxos-osdi16.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        
          <a href="https://syslab.cs.washington.edu/papers/nopaxos-osdi16-slides.pdf">
              <i class="fa fa-files-o"></i> slides
          </a>
        

        
      </span>

      
        <div id="abstract_nopaxos-osdi16" class="abstract">
          Distributed applications use replication, implemented by protocols like Paxos, to ensure data availability and transparently mask server failures. This paper presents a new approach to achieving replication in the data center without the performance cost of traditional methods. Our work carefully divides replication responsibility between the network and protocol layers. The network orders requests but does not ensure reliable delivery – using a new primitive we call ordered unreliable multicast (OUM). Implementing this primitive can be achieved with near-zero-cost in the data center. Our new replication protocol, Network-Ordered Paxos (NOPaxos), exploits network ordering to provide strongly consistent replication without coordination. The resulting system not only outperforms both latency- and throughput-optimized protocols on their respective metrics, but also yields throughput within 2% and latency within 16 μs of an unreplicated system – providing replication without the performance cost.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          
            <span class="month">September</span>
            
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Just Say NO to Paxos Overhead: Replacing Consensus with Network Ordering (Extended Version).</span><br>
      <span class="authors">Jialin Li, Ellis Michael, Adriana Szekeres, Naveen Kr. Sharma, and Dan R. K. Ports.
      
      </span>
      <br>

      
        <span class="venuetype">Technical Report</span>
        <span class="venue">UW-CSE-16-09-02,</span>
        <span class=venuelocation>University of Washington CSE.</span>
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_nopaxos-tr16">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://www.cs.washington.edu/tr/2016/09/UW-CSE-16-09-02.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_nopaxos-tr16" class="abstract">
          Distributed applications use replication, implemented by protocols like Paxos, to ensure data availability and transparently mask server failures. This paper presents a new approach to achieving replication in the data center without the performance cost of traditional methods. Our work carefully divides replication responsibility between the network and protocol layers. The network orders requests but does not ensure reliable delivery – using a new primitive we call ordered unreliable multicast (OUM). Implementing this primitive can be achieved with near-zero-cost in the data center. Our new replication protocol, Network-Ordered Paxos (NOPaxos), exploits network ordering to provide strongly consistent replication without coordination. The resulting system not only outperforms both latency- and throughput-optimized protocols on their respective metrics, but also yields throughput within 2% and latency within 16 μs of an unreplicated system – providing replication without the performance cost.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          
            <span class="month">August</span>
            
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Providing Stable Storage for the Diskless Crash-Recovery Failure Model.</span><br>
      <span class="authors">Ellis Michael, Dan R. K. Ports, Naveen Kr. Sharma, and Adriana Szekeres.
      
      </span>
      <br>

      
        <span class="venuetype">Technical Report</span>
        <span class="venue">UW-CSE-16-08-02,</span>
        <span class=venuelocation>University of Washington CSE.</span>
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_diskless-tr16">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="https://www.cs.washington.edu/tr/2016/08/UW-CSE-16-08-02.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_diskless-tr16" class="abstract">
          Many classic protocols in the fault tolerant distributed computing literature assume a Crash-Fail model in which processes either are up, or have crashed and are permanently down. While this model is useful, it does not fully capture the difficulties many real systems must contend with. In particular, real-world systems are long-lived and must have a recovery mechanism so that crashed processes can rejoin the system and restore its fault-tolerance. When processes are assumed to have access to stable storage that is persistent across failures, the Crash-Recovery model is trivial. However, because disk failures are common and because having a disk on a protocol’s critical path is often performance concern, diskless recovery protocols are needed. While such protocols do exist in the state machine replication literature, several well-known protocols have flawed recovery mechanisms. We examine these errors to elucidate the problem of diskless recovery and present our own protocol for providing virtual stable storage, transforming any protocol in the Crash-Recovery with stable storage model into a protocol in the Diskless Crash-Recover model.
        </div>
      
    </td>
  </tr>

  <tr>
    <td>
      <span class="date">
        
          <span class="year">2015</span><br>
          
          <span class="month">May</span>
          
        
      </span>
    </td>

    <td class="pub">
      <span class="title">Scaling Leader-Based Protocols for State Machine Replication.</span><br>
      <span class="authors">Ellis Michael.
      
        Supervisor: Lorenzo Alvisi.
      
      </span>
      <br>

      
        <span class="venuetype">Undergraduate Honors Thesis.</span>
        <span class="venue">University of Texas at Austin.</span>
      

      
      <br>
      <span class="links">
        
          <a class="abstract-toggle" href="javascript:void(0);" role="button"
             abstract="#abstract_thesis15">
               <i class="fa fa-chevron-down"></i> abstract
          </a>
        

        
          <a href="http://apps.cs.utexas.edu/apps/sites/default/files/tech_reports/TR-2197.pdf">
              <i class="fa fa-download"></i> pdf
          </a>
        

        

        
      </span>

      
        <div id="abstract_thesis15" class="abstract">
          State machine replication is a technique used to guarantee the availability of a system even in the presence of faults. Agreement protocols are often used to implement state machine replication. However, the throughput of many agreement protocols, such as Paxos, does not scale with the number of machines available to the system. Systems whose throughput does scale often provide weaker consistency guarantees than state machine replication’s linearizability. These weaker guarantees, such as eventual consistency, make the job of the application programmer much more difficult.<br />
Kapritsos and Junqueira previously proposed a protocol which uses an agreement protocol as a primitive and does scale. In this thesis, I describe my novel implementation of this protocol as well as the optimizations that were necessary to achieve scalability. The results of my experiments with this system show modest but promising evidence that the throughput of this system can, indeed, scale linearly with the number of machines.
        </div>
      
    </td>
  </tr>

</tbody>
</table>

  </div>
</div>

  </main>

  <footer>
    <div class="row">
      <div class="contact-info">
        <h3>Around the Web</h3>
        <ul>
          
            <li>
              <a href="https://github.com/emichael"><i title="GitHub" class="fa fa-fw fa-github"></i></a>
            </li>
          
            <li>
              <a href="https://scholar.google.com/citations?user=UPrphFkAAAAJ"><i title="Google Scholar" class="ai ai-fw ai-google-scholar"></i></a>
            </li>
          
            <li>
              <a href="https://www.linkedin.com/in/rellismichael"><i title="LinkedIn" class="fa fa-fw fa-linkedin"></i></a>
            </li>
          
            <li>
              <a href="/feed.xml"><i title="RSS Feed" class="fa fa-fw fa-rss"></i></a>
            </li>
          
        </ul>
      </div>
      <div class="contact-email">
        <h3>Email</h3>
        <p>(at ellis (dot ellismichael com))</p>
      </div>
    </div>

    <div class="row">
      <div class="copyright">
        Copyright &copy; Ellis Michael 2013&ndash;2025 |
        Last updated: April 14, 2025
      </div>
    </div>
  </footer>

  <!-- jQuery Version 1.11.0 -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>

  <!-- Bootstrap Core JavaScript -->
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>

  <!-- Custom JavaScript -->
  <script src="/js/main.js"></script>

  
  
</body>
</html>
