@inproceedings{hydra-nsdi23,
  author        = {Inho Choi and Ellis Michael and Yunfan Li and Dan R. K. Ports
                  and Jialin Li},
  title         = {Hydra: Serialization-Free Network Ordering for Strongly
                  Consistent Distributed Applications},
  booktitle     = {Proceedings of the 20th {USENIX} Symposium on Networked
                  Systems Design and Implementation ({NSDI} '23)},
  year          = {2023},
  month         = apr,
  location      = {Boston, MA, USA},
  url           = {https://ellismichael.com/papers/hydra-nsdi23.pdf},
  abstract      = {Many distributed systems, e.g., state machine replication and
                  distributed databases, rely on establishing a consistent order
                  of operations on groups of nodes in the system.
                  Traditionally, this ordering has been established by
                  application-level protocols like Paxos or two-phase locking.
                  Recent work has shown significant performance improvements
                  are attainable by making ordering a network service, but
                  current network sequencing implementations require routing
                  all requests through a single sequencer – leading to
                  scalability, fault tolerance, and load balancing
                  limitations.

                  Our work, Hydra, overcomes these limitations by using a
                  distributed set of network sequencers to provide network
                  ordering. Hydra leverages loosely synchronized clocks on
                  network sequencers to establish message ordering across them,
                  per-sequencer sequence numbers to detect message drops, and
                  periodic timestamp messages to enforce progress when some
                  sequencers are idle. To demonstrate the benefit of Hydra, we
                  co-designed a state machine replication protocol and a
                  distributed transactional system using the Hydra network
                  primitive. Compared to serialization-based network ordering
                  systems, Hydra shows equivalent performance improvement over
                  traditional approaches in both applications, but with
                  significantly higher scalability, shorter sequencer failover
                  time, and better network-level load balancing.}
}

@inproceedings{pegasus-osdi20,
  author        = {Jialin Li and Jacob Nelson and Ellis Michael and Xin Jin and
                  Dan R. K. Ports},
  title         = {Pegasus: Tolerating Skewed Workloads in Distributed Storage
                  with In-Network Coherence Directories},
  booktitle     = {Proceedings of the 14th {USENIX} Symposium on Operating
                  Systems Design and Implementation ({OSDI} '20)},
  year          = {2020},
  month         = nov,
  location      = {Banff, Alberta, Canada},
  url           = {https://ellismichael.com/papers/pegasus-osdi20.pdf},
  abstract      = {High performance distributed storage systems face the
                  challenge of load imbalance caused by skewed and dynamic
                  workloads. This paper introduces Pegasus, a new storage system
                  that leverages new-generation programmable switch ASICs to
                  balance load across storage servers. Pegasus uses selective
                  replication of the most popular objects in the data store to
                  distribute load. Using a novel in-network coherence directory,
                  the Pegasus switch tracks and manages the location of
                  replicated objects. This allows it to achieve load-aware
                  forwarding and dynamic rebalancing for replicated keys, while
                  still guaranteeing data coherence and consistency. The Pegasus
                  design is practical to implement as it stores only forwarding
                  metadata in the switch data plane. The resulting system
                  improves the 99% tail latency of a distributed in-memory
                  key-value store by more than 95%, and yields up to a 9x
                  throughput improvement under a latency SLO – results which
                  hold across a large set of workloads with varying degrees of
                  skew, read/write ratio, and dynamism.}
}

@inproceedings{dslabs-neted20,
  author        = {Tom Anderson and Ellis Michael},
  title         = {On the Use of Model Checking for Remote Instruction},
  booktitle     = {ACM SIGCOMM CCR Series on Networking Education},
  year          = {2020},
  month         = aug,
  url           = {https://ellismichael.com/papers/mchecking-sigcommedu20.pdf}
}

@inproceedings{harmonia-vldb20,
  author        = {Hang Zhu and Zhihao Bai and Jialin Li and Ellis Michael and
                  Dan R. K. Ports and Ion Stoica and Xin Jin},
  title         = {Harmonia: Near-Linear Scalability for Replicated Storage with
                  In-Network Conflict Detection},
  booktitle     = {Proceedings of the 46th International Conference on Very
                  Large Data Bases ({VLDB} '20)},
  year          = {2020},
  month         = aug,
  location      = {Tokyo, Japan},
  url           = {https://ellismichael.com/papers/harmonia-vldb19.pdf},
  abstract      = {Distributed storage employs replication to mask failures and
                  improve availability. However, these systems typically exhibit
                  a hard tradeoff between consistency and performance. Ensuring
                  consistency introduces coordination overhead, and as a result
                  the system throughput does not scale with the number of
                  replicas. We present Harmonia, a replicated storage
                  architecture that exploits the capability of new-generation
                  programmable switches to obviate this tradeoff by providing
                  near-linear scalability without sacrificing consistency. To
                  achieve this goal, Harmonia detects read-write conflicts in
                  the network, which enables any replica to serve reads for
                  objects with no pending writes. Harmonia implements this
                  functionality at line rate, thus imposing no performance
                  overhead. We have implemented a prototype of Harmonia on a
                  cluster of commodity servers connected by a Barefoot Tofino
                  switch, and have integrated it with Redis. We demonstrate the
                  generality of our approach by supporting a variety of
                  replication protocols, including primary-backup, chain
                  replication, Viewstamped Replication, and NOPaxos.
                  Experimental results show that Harmonia improves the
                  throughput of these protocols by up to 10x for a replication
                  factor of 10, providing near-linear scalability up to the
                  limit of our testbed.}
}

@techreport{harmonia-arxiv19,
  author        = {Hang Zhu and Zhihao Bai and Jialin Li and Ellis Michael and
                  Dan R. K. Ports and Ion Stoica and Xin Jin},
  title         = {Harmonia: Near-Linear Scalability for Replicated Storage with
                  In-Network Conflict Detection},
  number        = {1904.08964},
  institution   = {{arXiv}},
  url           = {https://arxiv.org/pdf/1904.08964.pdf},
  year          = {2019},
  month         = apr,
  abstract      = {Distributed storage employs replication to mask failures and
                  improve availability. However, these systems typically exhibit
                  a hard tradeoff between consistency and performance. Ensuring
                  consistency introduces coordination overhead, and as a result
                  the system throughput does not scale with the number of
                  replicas. We present Harmonia, a replicated storage
                  architecture that exploits the capability of new-generation
                  programmable switches to obviate this tradeoff by providing
                  near-linear scalability without sacrificing consistency. To
                  achieve this goal, Harmonia detects read-write conflicts in
                  the network, which enables any replica to serve reads for
                  objects with no pending writes. Harmonia implements this
                  functionality at line rate, thus imposing no performance
                  overhead. We have implemented a prototype of Harmonia on a
                  cluster of commodity servers connected by a Barefoot Tofino
                  switch, and have integrated it with Redis. We demonstrate the
                  generality of our approach by supporting a variety of
                  replication protocols, including primary-backup, chain
                  replication, Viewstamped Replication, and NOPaxos.
                  Experimental results show that Harmonia improves the
                  throughput of these protocols by up to 10x for a replication
                  factor of 10, providing near-linear scalability up to the
                  limit of our testbed.}
}

@inproceedings{dslabs-eurosys19,
  author        = {Ellis Michael and Doug Woos and Thomas Anderson and Michael
                  D. Ernst and Zachary Tatlock},
  title         = {Teaching Rigorous Distributed Systems With Efficient Model
                  Checking},
  booktitle     = {Proceedings of the 14th European Conference on Computer
                  Systems ({EuroSys} '19)},
  year          = {2019},
  month         = mar,
  location      = {Dresden, Germany},
  url           = {https://ellismichael.com/papers/dslabs-eurosys19.pdf},
  slides        = {https://ellismichael.com/papers/dslabs-eurosys19-slides.pdf},
  abstract      = {Writing correct distributed systems code is difficult,
                  especially for novice programmers. The inherent asynchrony and
                  need for fault-tolerance make errors almost inevitable.
                  Industrial-strength testing and model checking have been shown
                  to be effective at uncovering bugs, but they come at a cost
                  — in both time and effort — that is far beyond what
                  students can afford. To address this, we have developed an
                  efficient model checking framework and visual debugger for
                  distributed systems, with the goal of helping students find
                  and fix bugs in near real-time. We identify two novel
                  techniques for reducing the search state space to more
                  efficiently find bugs in student implementations. We report
                  our experiences using these tools to help over two hundred
                  students build a correct, linearizable, fault-tolerant,
                  dynamically-sharded key–value store.}
}

@inproceedings{tcdn-papoc18,
  author        = {Ellis Michael and Dan R. K. Ports},
  title         = {Towards Causal Datacenter Networks},
  booktitle     = {Proceedings of the 5th Workshop on Principles and Practice of
                  Consistency for Distributed Data ({PaPoC} '18)},
  year          = {2018},
  month         = apr,
  location      = {Porto, Portugal},
  url           = {https://syslab.cs.washington.edu/papers/tcdn-papoc18.pdf},
  abstract      = {Traditionally, distributed systems conservatively assume an
                  asynchronous network. However, recent work on the co-design of
                  networks and distributed systems has shown that stronger
                  ordering properties are achievable in datacenter networks and
                  yield performance improvements for the distributed systems
                  they support. We build on that trend and ask whether it is
                  possible for the datacenter network to order all messages in a
                  protocol-agnostic way. This approach, which we call
                  omnisequencing, would ensure causal delivery of all messages,
                  making consistency a network-level guarantee.}
}

@inproceedings{eris-sosp17,
  author        = {Jialin Li and Ellis Michael and Dan R. K. Ports},
  title         = {Eris: Coordination-Free Consistent Transactions Using
                  In-Network Concurrency Control},
  booktitle     = {Proceedings of the 26th {ACM} Symposium on Operating Systems
                  Principles ({SOSP} '17)},
  year          = {2017},
  month         = oct,
  location      = {Shanghai, China},
  url           = {https://syslab.cs.washington.edu/papers/eris-sosp17.pdf},
  slides        = {https://syslab.cs.washington.edu/papers/eris-sosp17-slides.pdf},
  video         = {https://dl.acm.org/ft_gateway.cfm?id=3132751&ftid=1940567&dwn=1},
  abstract      = {Distributed storage systems aim to provide strong consistency
                  and isolation guarantees on an architecture that is
                  partitioned across multiple shards for scalability and
                  replicated for fault tolerance. Traditionally, achieving all
                  of these goals has required an expensive combination of atomic
                  commitment and replication protocols – introducing extensive
                  coordination overhead. Our system, Eris, takes a different
                  approach. It moves a core piece of concurrency control
                  functionality, which we term multi-sequencing, into the
                  datacenter network itself. This network primitive takes on the
                  responsibility for consistently ordering transactions, and a
                  new lightweight transaction protocol ensures atomicity.

                  The end result is that Eris avoids both replication and
                  transaction coordination overhead: we show that it can process
                  a large class of distributed transactions in a single
                  round-trip from the client to the storage system without any
                  explicit coordination between shards or replicas in the normal
                  case. It provides atomicity, consistency, and fault tolerance
                  with less than 10% overhead – achieving throughput 3.6–35x
                  higher and latency 72–80% lower than a conventional design on
                  standard benchmarks.}
}

@techreport{eris-tr17,
  author        = {Jialin Li and Ellis Michael and Dan R. K. Ports},
  title         = {Eris: Coordination-Free Consistent Transactions Using
                  In-Network Concurrency Control (Extended Version)},
  institution   = {University of Washington CSE},
  year          = {2017},
  number        = {UW-CSE-17-10-01},
  address       = {Seattle, WA, USA},
  month         = oct,
  url           = {https://www.cs.washington.edu/tr/2017/10/UW-CSE-17-10-01.pdf},
  abstract      = {Distributed storage systems aim to provide strong consistency
                  and isolation guarantees on an architecture that is
                  partitioned across multiple shards for scalability and
                  replicated for fault tolerance. Traditionally, achieving all
                  of these goals has required an expensive combination of atomic
                  commitment and replication protocols -- introducing extensive
                  coordination overhead. Our system, Eris, takes a different
                  approach. It moves a core piece of concurrency control
                  functionality, which we term multi-sequencing, into the
                  datacenter network itself. This network primitive takes on the
                  responsibility for consistently ordering transactions, and a
                  new lightweight transaction protocol ensures atomicity.

                  The end result is that Eris avoids both replication and
                  transaction coordination overhead: we show that it can process
                  a large class of distributed transactions in a single
                  round-trip from the client to the storage system without any
                  explicit coordination between shards or replicas in the normal
                  case. It provides atomicity, consistency, and fault tolerance
                  with less than 10% overhead -- achieving throughput 3.6--35x
                  higher and latency 72--80% lower than a conventional design on
                  standard benchmarks.}
}

@inproceedings{recovering-disc17,
  author        = {Ellis Michael and Dan R. K. Ports and Naveen Kr. Sharma and
                  Adriana Szekeres},
  title         = {Recovering Shared Objects Without Stable Storage},
  booktitle     = {Proceedings of the 31st International Symposium on
                  Distributed Computing ({DISC} '17)},
  year          = {2017},
  month         = oct,
  location      = {Vienna, Austria},
  url           = {https://syslab.cs.washington.edu/papers/recovering-disc17.pdf},
  abstract      = {This paper considers the problem of building fault-tolerant
                  shared objects when processes can crash and recover but lose
                  their persistent state on recovery. This Diskless
                  Crash-Recovery (DCR) model matches the way many long-lived
                  systems are built. We show that it presents new challenges, as
                  operations that are recorded at a quorum may not persist after
                  some of the processes in that quorum crash and then recover.

                  To address this problem, we introduce the notion of
                  crash-consistent quorums, where no recoveries happen during
                  the quorum responses. We show that relying on crash-consistent
                  quorums enables a recovery procedure that can recover all
                  operations that successfully finished. Crash-consistent
                  quorums can be easily identified using a mechanism we term the
                  crash vector, which tracks the causal relationship between
                  crashes, recoveries, and other operations.

                  We apply crash-consistent quorums and crash vectors to build
                  two storage primitives. We give a new algorithm for
                  multi-writer, multi-reader atomic registers in the DCR model
                  that guarantees safety under all conditions and termination
                  under a natural condition. It improves on the best prior
                  protocol for this problem by requiring fewer rounds, fewer
                  nodes to participate in the quorum, and a less restrictive
                  liveness condition. We also present a more efficient
                  single-writer, single-reader atomic set---a virtual stable
                  storage abstraction. It can be used to lift any existing
                  algorithm from the traditional Crash-Recovery model to the DCR
                  model. We examine a specific application, state machine
                  replication, and show that existing diskless protocols can
                  violate their correctness guarantees, while ours offers a
                  general and correct solution.}
}

@techreport{recovering-tr17,
  author        = {Ellis Michael and Dan R. K. Ports and Naveen Kr. Sharma and
                  Adriana Szekeres},
  title         = {Recovering Shared Objects Without Stable Storage (Extended
                  Version)},
  year          = {2017},
  address       = {Seattle, WA, USA},
  month         = aug,
  url           = {https://www.cs.washington.edu/tr/2017/08/UW-CSE-17-08-01.pdf},
  institution   = {University of Washington CSE},
  number        = {UW-CSE-17-08-01},
  abstract      = {This paper considers the problem of building fault-tolerant
                  shared objects when processes can crash and recover but lose
                  their persistent state on recovery. This Diskless
                  Crash-Recovery (DCR) model matches the way many long-lived
                  systems are built. We show that it presents new challenges, as
                  operations that are recorded at a quorum may not persist after
                  some of the processes in that quorum crash and then recover.

                  To address this problem, we introduce the notion of
                  crash-consistent quorums, where no recoveries happen during
                  the quorum responses. We show that relying on crash-consistent
                  quorums enables a recovery procedure that can recover all
                  operations that successfully finished. Crash-consistent
                  quorums can be easily identified using a mechanism we term the
                  crash vector, which tracks the causal relationship between
                  crashes, recoveries, and other operations.

                  We apply crash-consistent quorums and crash vectors to build
                  two storage primitives. We give a new algorithm for
                  multi-writer, multi-reader atomic registers in the DCR model
                  that guarantees safety under all conditions and termination
                  under a natural condition. It improves on the best prior
                  protocol for this problem by requiring fewer rounds, fewer
                  nodes to participate in the quorum, and a less restrictive
                  liveness condition. We also present a more efficient
                  single-writer, single-reader atomic set---a virtual stable
                  storage abstraction. It can be used to lift any existing
                  algorithm from the traditional Crash-Recovery model to the DCR
                  model. We examine a specific application, state machine
                  replication, and show that existing diskless protocols can
                  violate their correctness guarantees, while ours offers a
                  general and correct solution.}
}

@inproceedings{nopaxos-osdi16,
  author        = {Jialin Li and Ellis Michael and Adriana Szekeres and Naveen
                  Kr. Sharma and Dan R. K. Ports},
  title         = {Just Say {NO} to {Paxos} Overhead: Replacing Consensus with
                  Network Ordering},
  booktitle     = {Proceedings of the 12th {USENIX} Symposium on Operating
                  Systems Design and Implementation ({OSDI} '16)},
  year          = {2016},
  address       = {Savannah, GA, USA},
  month         = nov,
  location      = {Savannah, GA, USA},
  url           = {https://syslab.cs.washington.edu/papers/nopaxos-osdi16.pdf},
  alturl        = {https://www.usenix.org/system/files/conference/osdi16/osdi16-li.pdf},
  slides        = {https://syslab.cs.washington.edu/papers/nopaxos-osdi16-slides.pdf},
  altslides     = {https://www.usenix.org/sites/default/files/conference/protected-files/osdi16_slides_li_jialin.pdf},
  abstract      = {Distributed applications use replication, implemented by
                  protocols like Paxos, to ensure data availability and
                  transparently mask server failures. This paper presents a new
                  approach to achieving replication in the data center without
                  the performance cost of traditional methods. Our work
                  carefully divides replication responsibility between the
                  network and protocol layers. The network orders requests but
                  does not ensure reliable delivery -- using a new primitive we
                  call ordered unreliable multicast (OUM). Implementing this
                  primitive can be achieved with near-zero-cost in the data
                  center. Our new replication protocol, Network-Ordered Paxos
                  (NOPaxos), exploits network ordering to provide strongly
                  consistent replication without coordination. The resulting
                  system not only outperforms both latency- and
                  throughput-optimized protocols on their respective metrics,
                  but also yields throughput within 2% and latency within 16 μs
                  of an unreplicated system -- providing replication without the
                  performance cost.}
}

@techreport{nopaxos-tr16,
  author        = {Jialin Li and Ellis Michael and Adriana Szekeres and Naveen
                  Kr. Sharma and Dan R. K. Ports},
  title         = {Just Say {NO} to {Paxos} Overhead: Replacing Consensus with
                  Network Ordering (Extended Version)},
  institution   = {University of Washington CSE},
  year          = {2016},
  number        = {UW-CSE-16-09-02},
  month         = sep,
  url           = {https://www.cs.washington.edu/tr/2016/09/UW-CSE-16-09-02.pdf},
  abstract      = {Distributed applications use replication, implemented by
                  protocols like Paxos, to ensure data availability and
                  transparently mask server failures. This paper presents a new
                  approach to achieving replication in the data center without
                  the performance cost of traditional methods. Our work
                  carefully divides replication responsibility between the
                  network and protocol layers. The network orders requests but
                  does not ensure reliable delivery -- using a new primitive we
                  call ordered unreliable multicast (OUM). Implementing this
                  primitive can be achieved with near-zero-cost in the data
                  center. Our new replication protocol, Network-Ordered Paxos
                  (NOPaxos), exploits network ordering to provide strongly
                  consistent replication without coordination. The resulting
                  system not only outperforms both latency- and
                  throughput-optimized protocols on their respective metrics,
                  but also yields throughput within 2% and latency within 16 μs
                  of an unreplicated system -- providing replication without the
                  performance cost.}
}

@techreport{diskless-tr16,
  author        = {Ellis Michael and Dan R. K. Ports and Naveen Kr. Sharma and
                  Adriana Szekeres},
  title         = {Providing Stable Storage for the Diskless Crash-Recovery
                  Failure Model},
  institution   = {University of Washington CSE},
  year          = {2016},
  number        = {UW-CSE-16-08-02},
  address       = {Seattle, WA, USA},
  month         = aug,
  url           = {https://www.cs.washington.edu/tr/2016/08/UW-CSE-16-08-02.pdf},
  abstract      = {Many classic protocols in the fault tolerant distributed
                  computing literature assume a Crash-Fail model in which
                  processes either are up, or have crashed and are permanently
                  down. While this model is useful, it does not fully capture
                  the difficulties many real systems must contend with. In
                  particular, real-world systems are long-lived and must have a
                  recovery mechanism so that crashed processes can rejoin the
                  system and restore its fault-tolerance. When processes are
                  assumed to have access to stable storage that is persistent
                  across failures, the Crash-Recovery model is trivial. However,
                  because disk failures are common and because having a disk on
                  a protocol's critical path is often performance concern,
                  diskless recovery protocols are needed. While such protocols
                  do exist in the state machine replication literature, several
                  well-known protocols have flawed recovery mechanisms. We
                  examine these errors to elucidate the problem of diskless
                  recovery and present our own protocol for providing virtual
                  stable storage, transforming any protocol in the
                  Crash-Recovery with stable storage model into a protocol in
                  the Diskless Crash-Recover model.}
}

@thesis{thesis15,
  author        = {Ellis Michael},
  title         = {Scaling Leader-Based Protocols for State Machine
                  Replication},
  type          = {Undergraduate Honors Thesis},
  year          = {2015},
  month         = may,
  supervisor    = {Lorenzo Alvisi},
  institution   = {University of Texas at Austin},
  url           = {http://apps.cs.utexas.edu/apps/sites/default/files/tech_reports/TR-2197.pdf},
  alturl        = {http://repositories.lib.utexas.edu/bitstream/handle/2152/29985/ellis_michael_thesis.pdf},
  abstract      = {State machine replication is a technique used to guarantee
                  the availability of a system even in the presence of faults.
                  Agreement protocols are often used to implement state machine
                  replication. However, the throughput of many agreement
                  protocols, such as Paxos, does not scale with the number of
                  machines available to the system. Systems whose throughput
                  does scale often provide weaker consistency guarantees than
                  state machine replication's linearizability. These weaker
                  guarantees, such as eventual consistency, make the job of the
                  application programmer much more difficult.

                  Kapritsos and Junqueira previously proposed a protocol which
                  uses an agreement protocol as a primitive and does scale. In
                  this thesis, I describe my novel implementation of this
                  protocol as well as the optimizations that were necessary to
                  achieve scalability. The results of my experiments with this
                  system show modest but promising evidence that the throughput
                  of this system can, indeed, scale linearly with the number of
                  machines.}
}
